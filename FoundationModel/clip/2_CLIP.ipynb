{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75e645-50c2-4c79-a457-1d48ad7fa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import CLIPProcessor, CLIPModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d58d22-b1d8-4f31-b16f-0c43b418e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"openai/clip-vit-base-patch16\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(checkpoint)\n",
    "processor = CLIPProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394ec34-1043-4a76-96c8-cfc0f9ca0f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!tar -xvf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f69bb2-7c3b-4847-95b2-b1212f1406c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"./VOCdevkit/VOC2012/JPEGImages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427ee96-9ad7-4b36-bcab-8144c7bda9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "seed = 221\n",
    "candidate_subset = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419c7cf-2bb5-47c0-a7c1-210e09f3ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_subset['image'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb55224-a77a-41d0-836e-5f43de9fa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(candidate_subset):\n",
    "    inputs = processor(images=example['image'], text=['baby', 'car', 'men', 'bus'],  return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image[0]\n",
    "    probs = logits.softmax(dim=-1).detach().numpy()\n",
    "    scores = probs.tolist()\n",
    "\n",
    "    result = [\n",
    "    {\"score\": score, \"label\": candidate_label}\n",
    "    for score, candidate_label in sorted(zip(probs, ['baby', 'car', 'men', 'bus']), key=lambda x: -x[0])]\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421458d-933e-4f80-9731-4f7e5712e541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
