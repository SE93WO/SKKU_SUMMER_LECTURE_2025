{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d75bf6-5711-4a54-8738-47aa9b7c320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "\n",
    "# Third-party library imports\n",
    "import cv2  # OpenCV library for image processing\n",
    "import torch  # PyTorch library for tensor operations and deep learning\n",
    "import requests  # Requests library for making HTTP requests\n",
    "import numpy as np  # NumPy library for numerical operations\n",
    "from PIL import Image  # Pillow library for image handling\n",
    "import plotly.express as px  # Plotly Express for simple plotting\n",
    "import matplotlib.pyplot as plt  # Matplotlib for plotting\n",
    "import plotly.graph_objects as go  # Plotly for more complex plotting\n",
    "\n",
    "# Importing classes and functions from the transformers library\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1dc75e-ad1e-4691-a68b-c68698e8c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412cfba8-df2b-4871-aa7d-9f673797214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(image: Union[Image.Image, np.ndarray], detection_results: List[DetectionResult]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Annotate the image with bounding boxes and masks from detection results.\n",
    "    \n",
    "    :param image: Input image, either as a PIL Image or an OpenCV image (numpy array).\n",
    "    :param detection_results: List of DetectionResult objects containing detection details.\n",
    "    :return: Annotated image as a numpy array in RGB format.\n",
    "    \"\"\"\n",
    "    # Convert PIL Image to OpenCV format if necessary\n",
    "    image_cv2 = np.array(image) if isinstance(image, Image.Image) else image\n",
    "    # Convert color from RGB to BGR for OpenCV compatibility\n",
    "    image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Iterate over detections and add bounding boxes and masks\n",
    "    for detection in detection_results:\n",
    "        label = detection.label  # Get the label of the detection\n",
    "        score = detection.score  # Get the score of the detection\n",
    "        box = detection.box  # Get the bounding box of the detection\n",
    "        mask = detection.mask  # Get the mask of the detection, if available\n",
    "\n",
    "        # Sample a random color for each detection\n",
    "        color = np.random.randint(0, 256, size=3)\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(image_cv2, (box.xmin, box.ymin), (box.xmax, box.ymax), color.tolist(), 2)\n",
    "        # Put label and score text above the bounding box\n",
    "        cv2.putText(image_cv2, f'{label}: {score:.2f}', (box.xmin, box.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color.tolist(), 2)\n",
    "\n",
    "        # If mask is available, apply it\n",
    "        if mask is not None:\n",
    "            # Convert mask to uint8 format\n",
    "            mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Create a mask for filling the inside of the contours\n",
    "            mask_filled = np.zeros_like(image_cv2)\n",
    "            \n",
    "            # Fill the inside of the contours with the specified color\n",
    "            cv2.fillPoly(mask_filled, contours, color.tolist())\n",
    "            \n",
    "            # Apply the filled mask to the original image\n",
    "            # Create a condition to replace the image's color only inside the contours\n",
    "            mask_indices = mask_filled.astype(bool)\n",
    "            image_cv2[mask_indices] = mask_filled[mask_indices]\n",
    "                        \n",
    "    # Convert the image back to RGB format\n",
    "    return cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def plot_detections(\n",
    "    image: Union[Image.Image, np.ndarray],\n",
    "    detections: List[DetectionResult],\n",
    "    save_name: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the detections on the image and display it. Optionally, save the annotated image.\n",
    "    \n",
    "    :param image: Input image, either as a PIL Image or an OpenCV image (numpy array).\n",
    "    :param detections: List of DetectionResult objects containing detection details.\n",
    "    :param save_name: Optional name for saving the annotated image.\n",
    "    \"\"\"\n",
    "    # Annotate the image with detection results\n",
    "    annotated_image = annotate(image, detections)\n",
    "    \n",
    "    # Display the annotated image using matplotlib\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    \n",
    "    # Save the annotated image if a save name is provided\n",
    "    if save_name:\n",
    "        plt.savefig(save_name, bbox_inches='tight')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f5f71-b7b7-49e8-b61d-7183f2033121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_str: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a URL or local file path.\n",
    "\n",
    "    :param image_str: URL or local file path to the image.\n",
    "    :return: Loaded image as a PIL Image object in RGB format.\n",
    "    \"\"\"\n",
    "    if image_str.startswith(\"http\"):\n",
    "        # Load image from URL\n",
    "        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n",
    "    else:\n",
    "        # Load image from local file path\n",
    "        image = Image.open(image_str).convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_boxes(results: List[DetectionResult]) -> List[List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Extract bounding box coordinates from detection results.\n",
    "\n",
    "    :param results: List of DetectionResult objects.\n",
    "    :return: List of bounding box coordinates in [xmin, ymin, xmax, ymax] format.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for result in results:\n",
    "        # Get bounding box coordinates as [xmin, ymin, xmax, ymax]\n",
    "        xyxy = result.box.xyxy\n",
    "        boxes.append(xyxy)\n",
    "\n",
    "    return [boxes]\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Refine and convert boolean masks to uint8 format.\n",
    "\n",
    "    :param masks: Torch BoolTensor containing masks.\n",
    "    :param polygon_refinement: Flag for polygon refinement (default is False).\n",
    "    :return: List of refined masks as numpy arrays.\n",
    "    \"\"\"\n",
    "    # Move masks to CPU and convert to float\n",
    "    masks = masks.cpu().float()\n",
    "    # Permute dimensions to match (batch_size, height, width, channels)\n",
    "    masks = masks.permute(0, 2, 3, 1)\n",
    "    # Average over the last dimension to combine channels\n",
    "    masks = masks.mean(axis=-1)\n",
    "    # Threshold masks to binary format\n",
    "    masks = (masks > 0).int()\n",
    "    # Convert masks to numpy arrays and change dtype to uint8\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    # Convert to list of numpy arrays\n",
    "    masks = list(masks)\n",
    "\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0490b7d-cde4-4167-8140-531c6f57bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(\n",
    "    image: Image.Image,\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    detector: Optional[str] = None \n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
    "    \n",
    "    :param image: Input image as a PIL Image.\n",
    "    :param labels: List of labels to detect in the image.\n",
    "    :param threshold: Detection threshold.\n",
    "    :param detector: Optional model identifier for the object detector.\n",
    "    :return: List of detection results as dictionaries.\n",
    "    \"\"\"\n",
    "    # Set default detector if none is provided\n",
    "    detector = detector if detector is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
    "    \n",
    "    # Initialize object detector pipeline\n",
    "    object_detector = pipeline(model=detector,\n",
    "                               task=\"zero-shot-object-detection\", \n",
    "                               device='cuda')\n",
    "\n",
    "    # Ensure labels end with a period\n",
    "    labels = [label if label.endswith(\".\") else label + \".\" for label in labels]\n",
    "\n",
    "    # Perform object detection\n",
    "    results = object_detector(image, candidate_labels=labels, threshold=threshold)\n",
    "    \n",
    "    # Convert detection results to DetectionResult objects\n",
    "    results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "def segment(\n",
    "    image: Image.Image,\n",
    "    detection_results: List[Dict[str, Any]],\n",
    "    segmenter: Optional[str] = None\n",
    ") -> List[DetectionResult]:\n",
    "    \"\"\"\n",
    "    Use Segment Anything (SAM) to generate masks given an image and a set of bounding boxes.\n",
    "    \n",
    "    :param image: Input image as a PIL Image.\n",
    "    :param detection_results: List of detection results as dictionaries.\n",
    "    :param segmenter: Optional model identifier for the segmenter.\n",
    "    :return: List of DetectionResult objects with masks.\n",
    "    \"\"\"\n",
    "    # Set default segmenter if none is provided\n",
    "    segmenter = segmenter if segmenter is not None else \"facebook/sam-vit-base\"\n",
    "\n",
    "    # Initialize segmentator model and processor\n",
    "    segmentator = AutoModelForMaskGeneration.from_pretrained('facebook/sam-vit-base').to('cuda')\n",
    "    processor = AutoProcessor.from_pretrained('facebook/sam-vit-base')\n",
    "\n",
    "    # Get bounding boxes from detection results\n",
    "    boxes = get_boxes(detection_results)\n",
    "    \n",
    "    # Prepare inputs for the segmentator\n",
    "    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generate masks using the segmentator\n",
    "    outputs = segmentator(**inputs)\n",
    "    \n",
    "    # Post-process masks to match the original image size\n",
    "    masks = processor.post_process_masks(\n",
    "        masks=outputs.pred_masks,\n",
    "        original_sizes=inputs.original_sizes,\n",
    "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "    )[0]\n",
    "\n",
    "    # Refine masks\n",
    "    masks = refine_masks(masks)\n",
    "\n",
    "    # Assign masks to corresponding detection results\n",
    "    for detection_result, mask in zip(detection_results, masks):\n",
    "        detection_result.mask = mask\n",
    "\n",
    "    return detection_results\n",
    "\n",
    "def grounded_segmentation(\n",
    "    image: Union[Image.Image, str],\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    detector: Optional[str] = None,\n",
    "    segmenter: Optional[str] = None\n",
    ") -> Tuple[np.ndarray, List[DetectionResult]]:\n",
    "    \"\"\"\n",
    "    Perform grounded segmentation by detecting objects and generating masks in an image.\n",
    "    \n",
    "    :param image: Input image as a PIL Image or file path (str).\n",
    "    :param labels: List of labels to detect in the image.\n",
    "    :param threshold: Detection threshold.\n",
    "    :param detector: Optional model identifier for the object detector.\n",
    "    :param segmenter: Optional model identifier for the segmenter.\n",
    "    :return: Tuple of annotated image as numpy array and list of DetectionResult objects.\n",
    "    \"\"\"\n",
    "    # Load image if a file path is provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "\n",
    "    # Detect objects in the image\n",
    "    detections = detect(image, labels, threshold, detector)\n",
    "    \n",
    "    # Generate masks for detected objects\n",
    "    detections = segment(image, detections, segmenter)\n",
    "\n",
    "    return np.array(image), detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e91d2f-a0af-4596-9cc2-899fa86382c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"goodfellowliu/City100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08ff2e-e517-4b00-82f3-18870175b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7fcef-67a5-4957-8dde-ee54beb94d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "seed = 221\n",
    "candidate_subset = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d6fa8-b132-4081-923e-37ee8afe4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sky\"]\n",
    "threshold = 0.3\n",
    "\n",
    "detector = \"IDEA-Research/grounding-dino-tiny\"\n",
    "segmenter = \"facebook/sam-vit-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e6d56-18bd-4ade-85b0-1cf0f6911258",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(candidate_subset):\n",
    "    image_array, detections = grounded_segmentation(\n",
    "    image=example['image'],\n",
    "    labels=labels,\n",
    "    threshold=threshold,\n",
    "    detector=detector,\n",
    "    segmenter=segmenter)\n",
    "\n",
    "    plot_detections(image_array, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db036aa-809c-4f18-84e2-b4700609bd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
