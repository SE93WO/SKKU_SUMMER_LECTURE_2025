{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "155a79b2-ef79-4867-bdef-ab50a932a677",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Image captioning\n",
    "\n",
    "Image captioning is the task of predicting a caption for a given image. Common real world applications of it include aiding visually impaired people that can help them navigate through different situations. Therefore, image captioning helps to improve content accessibility for people by describing images to them.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60653d07-8e85-4c81-84d5-7efe2c516558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'image-caption-blip-for-training' dataset from the 'JerryMo' repository on the Hugging Face datasets hub\n",
    "dataset = load_dataset(\"JerryMo/image-caption-blip-for-training\")\n",
    "\n",
    "# Display the loaded dataset\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfadbda-f157-48b1-9a1c-472a24b874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041bda7-9d2a-4640-aff8-cc9902b0a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c9e5d-4ab8-4f7f-be3c-a94178996503",
   "metadata": {},
   "source": [
    "# Preprocess the dataset\n",
    "\n",
    "Since the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.\r\n",
    "\r\n",
    "To do so, load the processor class associated with the model you are about to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08bc38-c2cd-4726-9cd2-eeffe1303d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# Define the model checkpoint for the processor\n",
    "checkpoint = \"microsoft/git-base\"\n",
    "\n",
    "# Load the processor from the specified pretrained model checkpoint\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529148d3-e755-4149-aba9-2e3d397da1a7",
   "metadata": {},
   "source": [
    "The processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708c549-b2e6-4714-9ab2-f585f85ef02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    \"\"\"\n",
    "    Apply transformations to a batch of examples by processing images and captions.\n",
    "    \n",
    "    :param example_batch: A batch of examples containing images and text captions.\n",
    "    :return: A dictionary of processed inputs with labels.\n",
    "    \"\"\"\n",
    "    # Extract images from the example batch\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    \n",
    "    # Extract captions from the example batch\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    \n",
    "    # Process the images and captions using the processor\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    \n",
    "    # Update the inputs dictionary to include labels (input IDs)\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Set the transform function to be applied to the dataset\n",
    "dataset.set_transform(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ec36c-f34a-469f-89cd-5f20aa059524",
   "metadata": {},
   "source": [
    "## Load a base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed97167-71b0-448c-871c-7fad6a8453e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load a pretrained causal language model from the specified checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5049b1c-de68-4c7e-8b89-521da6781e22",
   "metadata": {},
   "source": [
    "## \n",
    "Evaluate \n",
    "\n",
    "Image captioning models are typically evaluated with the Rouge Score or Word Error Rate. For this guide, you will use the Word Error Rate (WER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09034364-4bd3-4855-a76e-f8a68562f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load the Word Error Rate (WER) metric from the 'evaluate' library\n",
    "wer = load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics, specifically the Word Error Rate (WER), for the model predictions.\n",
    "\n",
    "    :param eval_pred: A tuple containing logits and labels from the model evaluation.\n",
    "    :return: A dictionary with the computed WER score.\n",
    "    \"\"\"\n",
    "    # Unpack logits and labels from the evaluation predictions\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Get the predicted token indices by taking the argmax over the last dimension of logits\n",
    "    predicted = logits.argmax(-1)\n",
    "    \n",
    "    # Decode the labels and predictions to text, skipping special tokens\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute the WER score using the decoded predictions and labels\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    \n",
    "    # Return the WER score in a dictionary\n",
    "    return {\"wer_score\": wer_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fa4ef-1144-40d0-a257-688f85b61790",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd0a2b-60de-4415-82ab-1331851039ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Extract the model name from the checkpoint string\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-simson\",        # Output directory for model checkpoints and logs\n",
    "    learning_rate=5e-5,                       # Learning rate for training\n",
    "    num_train_epochs=5,                      # Number of training epochs\n",
    "    fp16=True,                                # Use 16-bit (mixed) precision training\n",
    "    per_device_train_batch_size=8,           # Batch size per device (GPU/TPU) during training\n",
    "    gradient_accumulation_steps=2,            # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    save_total_limit=3,                       # Limit the total amount of checkpoints, deletes the older checkpoints\n",
    "    save_strategy=\"steps\",                    # Save model checkpoint strategy (either \"steps\" or \"epoch\")\n",
    "    save_steps=50,                            # Save model checkpoint every 50 steps\n",
    "    logging_steps=50,                         # Log training information every 50 steps\n",
    "    remove_unused_columns=False,              # Keep all columns in the dataset (don't remove unused columns)\n",
    "    label_names=[\"labels\"],                   # List of keys in your dictionary of inputs that correspond to the labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed0558-5a7b-408b-9ded-3ec13341d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                              # The model to be trained\n",
    "    args=training_args,                       # The training arguments\n",
    "    train_dataset=dataset,                    # The dataset to be used for training\n",
    "    compute_metrics=compute_metrics,          # The function to compute metrics during evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6becbf-f634-4dc1-b8b6-794497310786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72139fb-622d-4bbc-9bc8-db1a8520abd2",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fc9f3-6ec0-408b-85a3-3cbe70dd304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://visla.kr/wp/wp-content/uploads/2023/11/231106_02.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816510b-ee4a-47ca-b59c-c03dfa59c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7e126-9928-4b81-8cc7-127cbd45f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e83ff-6197-401b-8add-22ac8c7ada77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
