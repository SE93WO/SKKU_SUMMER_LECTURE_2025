{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd53c2d8-0d93-44f4-849b-42905c7824b5",
   "metadata": {},
   "source": [
    "# Models\n",
    "Transformers currently includes a large amount of vision models, for various tasks\n",
    "\n",
    "## Load a model\n",
    "\n",
    "Instantiating a model without pre-trained weights can be done by 1) instantiating a configuration, defining the model architecture 2) creating a model based on that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0025d7-8647-4083-b23a-35503d5547c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "\n",
    "config = ViTConfig(num_hidden_layers=12, hidden_size=768)\n",
    "model = ViTForImageClassification(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c54f13-2703-4967-97b1-87d5cefc4a8f",
   "metadata": {},
   "source": [
    "The configuration just stores the hyperparameters related to the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80148a-2913-4072-ab05-c7ce21808298",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03a82e-6f92-4992-8d51-bd36839073b3",
   "metadata": {},
   "source": [
    "Alternatively, (and this is what most people use), is to equip a model with pre-trained weights, such that it can be easily fine-tuned on a custom dataset.\n",
    "\n",
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11139d4f-ea35-4bd1-adb6-1782a8591929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a205811-363a-4fa6-8e4c-96af878df217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\",\n",
    "                                                  revision=\"db75733ce9ead4ed3dce26ab87a6ed2f6f565985\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53ba5-5b33-4dc5-82de-7e538c5681b7",
   "metadata": {},
   "source": [
    "## Load a feature extractor\r\n",
    "\r\n",
    "A feature extractor can be used to prepare images for the model.\r\n",
    "\r\n",
    "It's a minimal object to prepare images for inference.\r\n",
    "\r\n",
    "It typically does some very simple image transformations (like resizing to fixed size + normalizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8488fbf-bda6-44a8-b5bb-f4d1a867e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor \n",
    "\n",
    "feature_extractor = ViTImageProcessor ()\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921fdb3-9163-4cf9-9e60-ef5eefc5e09d",
   "metadata": {},
   "source": [
    "## Predict on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fdee6-25d1-471a-bf3c-6cb31deaf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image.save(\"cats.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859948e-02d4-44d5-9313-efc25af882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for the model\n",
    "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940bec7-d3bf-4962-87f6-1ab4817b29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = model(pixel_values)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49c2d2-02c9-4978-98d5-0bfb914c706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take argmax on logits' last dimension\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "# turn into actual class name\n",
    "print(model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b3119-e2ce-4a23-ab11-3e54e683dddc",
   "metadata": {},
   "source": [
    "## Auto API\n",
    "\n",
    "The Auto Classes automatically instantiate the appropriate class for you, based on the checkpoint identifier you provide.  \n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091573a-1237-41ef-ae2c-9400cdeb53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebba86-457e-4bd1-b308-218316fbaf34",
   "metadata": {},
   "source": [
    "## Image classification pipeline\r\n",
    "\r\n",
    "Image classification is probably the simplest vision task: given an image, predict which class(es) belong to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01da6db-d856-4c83-8712-c4dc0b627913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_pipe = pipeline(\"image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72153a6b-45bb-4e44-8127-fb04a447ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a8cbf-33cb-4803-88c4-a8d89690cfdd",
   "metadata": {},
   "source": [
    "Note that you can also provide a custom model from the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fdfe1-2c13-486b-82a7-ce2bc840f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe = pipeline(\"image-classification\", \n",
    "                      model=\"microsoft/swin-tiny-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b62df5-1b47-4de2-9574-31263c18cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37196b8-6dcb-4311-8c6a-02c337ffcbea",
   "metadata": {},
   "source": [
    "model + feature extractor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7455740-822f-41b5-af23-68c08cf28477",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/convnext-tiny-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")\n",
    "\n",
    "image_pipe = pipeline(\"image-classification\", \n",
    "                      model=model,\n",
    "                      feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06a693-7962-4c8e-99e5-e787e8d9051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34af105-c206-40ff-a5d4-dcabd100bb76",
   "metadata": {},
   "source": [
    "## Object detection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e518dbb-1534-466d-b705-0c34667fd978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_detection_pipe = pipeline(\"object-detection\",\n",
    "                                model=\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3d646-3af0-41e0-9f6b-730a166f2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = object_detection_pipe(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26560b-09a0-425f-86f2-da128a1aacd7",
   "metadata": {},
   "source": [
    "Visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24690c-d94c-48ea-bd78-b1c4368bdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125], [0.494, 0.184, 0.556], [0.466, 0.674, 0.188]]\n",
    "\n",
    "def plot_results(image, results):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for result, color in zip(results, colors):\n",
    "        box = result['box']\n",
    "        xmin, xmax, ymin, ymax = box['xmin'], box['xmax'], box['ymin'], box['ymax']\n",
    "        label = result['label']\n",
    "        prob = result['score']\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=color, linewidth=3))\n",
    "        text = f'{label}: {prob:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b95d6-5cf0-4cef-b6fa-1ba5f8ecdf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(image, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69538479-964e-48a2-bd1a-12f2ad058f66",
   "metadata": {},
   "source": [
    "## Depth estimation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac414d72-070a-4fdf-99f7-874df37a3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimation_pipe = pipeline(\"depth-estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d78f58-6821-48fb-976e-9d127ec1c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = depth_estimation_pipe(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1924de-9ea5-412a-9b64-b73ac0ea90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['depth'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec446ee3-5948-4060-8c13-59210b92aa91",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Loading a dataset from the hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305518cc-5abb-4ce0-872f-b74c8de8801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"cifar100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ac849-a85b-49c7-a64f-835ce465d51e",
   "metadata": {},
   "source": [
    "## Image feature\n",
    "\n",
    "You can directly view images in a notebook, as the images are of type Image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2c38f-3056-461f-98f6-55461cd5e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df7c07-8723-42a1-9cc2-a2eece04ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a796333-06e7-41a8-8075-41f296ccd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id: label for id, label in enumerate(dataset['train'].features['fine_label'].names)}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c6386-2c21-4153-899e-e0168a4672b9",
   "metadata": {},
   "source": [
    "## MultiModal\n",
    "\n",
    "Multi-modal models combine several modalities (e.g. language, vision, audio,...)\r\n",
    "* AI models are getting more powerful due to this! Humans also capture several modalities at the same time.\r\n",
    "* in Transformer, a so-calle `Processr` can be used to prepare the inputs for a model. Internally, a processor combines a tokenizer (for the text modality) and a  feature extractor (for the image/audio modality) for a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a73c-c4de-430a-81a2-fca40dc5485f",
   "metadata": {},
   "source": [
    "## Visual question answering (VQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4ef50-6412-419c-8ae7-ae4180217deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d0860-0787-42cc-8319-1e2b51b4d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how many cats are there?\"\n",
    "\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "print(encoding.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6efa7-4676-4e19-9eb1-cb1164b9caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = model(**encoding)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23e56a-3a69-4ded-997d-aff459819146",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted answer:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516306e-b04f-4324-91d7-63f63a793731",
   "metadata": {},
   "source": [
    "## Vision Encoder-Decoder Model\n",
    "\n",
    "Allows to use any Transformer-based vision encoder (e.g. ViT, Swin, BEiT) with any language decoder (e.g., BERT, GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b11e31-a181-482d-a83b-ee8eea35ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, ViTFeatureExtractor, VisionEncoderDecoderModel\n",
    "\n",
    "repo_name = \"ydshieh/vit-gpt2-coco-en\"\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(repo_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ba165-ebad-443d-9443-bf96fb7cdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# autoregressively generate text (using beam search or other decoding strategy)\n",
    "generated_ids = model.generate(pixel_values, max_length=16, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e97417-d922-47e2-8d44-32d9ebd80475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode into text\n",
    "preds = tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)\n",
    "preds = [pred.strip() for pred in preds]\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
